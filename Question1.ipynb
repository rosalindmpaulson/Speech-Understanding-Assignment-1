{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Speech Emotion Recognition\n",
    "\n",
    "Speech Emotion Recognition (SER) is a key area of speech processing that focuses on detecting and classifying human emotions from spoken language. It has applications in customer service automation, mental health monitoring, human-computer interaction, and more. The ability to correctly interpret human emotions enhances user experience and improves AI-driven systems.\n",
    "\n",
    "## 1. Task Explanation & Importance\n",
    "\n",
    "Speech Emotion Recognition (SER) involves identifying human emotions (e.g., happiness, anger, sadness) from vocal features like pitch, tone, and intensity. It is critical for:\n",
    "\n",
    "- **Mental health monitoring**: Detecting depression or anxiety from speech patterns (e.g., the Vietnamese Dynamic Attention-GRU model for depression diagnosis).\n",
    "- **Human-computer interaction**: Enabling empathetic AI in chatbots, virtual assistants, and robotics.\n",
    "- **Customer service**: Analyzing call center interactions to improve user experience.\n",
    "- **Healthcare**: Assisting in autism therapy or PTSD treatment by tracking emotional states.\n",
    "\n",
    "# 2. Importance in the Real World\n",
    "\n",
    "SER plays a crucial role in various domains:\n",
    "\n",
    "- **Healthcare**: Detecting depression, stress, or other emotional disorders.\n",
    "- **Customer Support**: Identifying frustrated or dissatisfied customers to improve service quality.\n",
    "- **Human-Robot Interaction**: Enhancing AI and robotic systems to respond empathetically.\n",
    "- **Security & Surveillance**: Identifying distress in emergency call systems.\n",
    "\n",
    "# 3. State-of-the-Art (SOTA) Models\n",
    "\n",
    "Several advanced models and techniques have been developed for SER:\n",
    "\n",
    "## a. Deep Learning-Based Models\n",
    "\n",
    "- **Convolutional Neural Networks (CNNs)**: Extracts local features from speech spectrograms.\n",
    "- **Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM)**: Captures temporal dependencies in speech signals.\n",
    "- **Transformer-based Models**: Such as Wav2Vec and Speech2Text models, achieving superior results.\n",
    "\n",
    "## b. Hybrid Approaches\n",
    "\n",
    "- **CNN-LSTM**: Combines CNN for feature extraction with LSTM for temporal modeling.\n",
    "- **Pretrained Models**: BERT-like architectures (e.g., Wav2Vec 2.0) for feature-rich embeddings.\n",
    "\n",
    "\n",
    "# 4. Strengths and Limitations of SOTA Models\n",
    "\n",
    "| Model                  | Strengths                              | Limitations                                |\n",
    "|------------------------|----------------------------------------|--------------------------------------------|\n",
    "| **CNN**                | Efficient for feature extraction       | Struggles with long-term dependencies      |\n",
    "| **RNN/LSTM**           | Good for sequential data processing    | Computationally expensive                  |\n",
    "| **Transformers**       | Superior accuracy, less manual feature engineering | Requires large-scale data and high computational power |\n",
    "| **CNN-LSTM**           | Effective for dynamic speech patterns  | Complexity increases training time         |\n",
    "| **Dynamic Attention-GRU** | High accuracy for specific tasks    | Limited cross-lingual generalization       |\n",
    "| **MDRE**               | Resolves class bias with multimodal fusion | Heavy computational requirements          |\n",
    "| **emotion2vec**        | Generalizes across languages and tasks | High pre-training data and computational cost |\n",
    "\n",
    "## 4.1 Comparative Analysis of Recent Models\n",
    "\n",
    "| Title                          | Year | Dataset(s)                                    | Models                                      | Performance Metrics         |\n",
    "|--------------------------------|------|-----------------------------------------------|---------------------------------------------|-----------------------------|\n",
    "| Ensemble 1D                    | 2021 | TESS, EMO-DB, RAVDESS, SAVEE, CREMA-D         | DNN, CNN, Ensemble                          | Precision, Recall, Accuracy |\n",
    "| Novel approach                 | 2022 | TESS, SAVEE, RAVDESS (Survey)                 | -                                           | -                           |\n",
    "| Wavelet Packet                 | 2020 | RAVDESS, SUSAS, ESD                           | Gradient Boosting, Random Forest            | Accuracy                    |\n",
    "| Selective Interpolation        | 2020 | SAVEE, EMO-DB                                 | SVM, Decision Tree, KNN, LR, Naive Bayes    | Precision, Recall, Accuracy |\n",
    "| Two way feature                | 2022 | RAVDESS, TESS                                 | Decision Tree, Random Forest, MLP, Proposed | Accuracy                    |\n",
    "| A real time emotion            | 2019 | RAVDESS, SAVEE                                | SVM, KNN, Gradient Boosting                 | Accuracy                    |\n",
    "| 3D CNN                         | 2019 | SAVEE, RML                                    | SVM, Random Forest, 2D CNN, Proposed        | Accuracy                    |\n",
    "| SER using clustering           | 2021 | RAVDESS, SAVEE                                | SVM                                         | Accuracy                    |\n",
    "| Clustering based Deep BiLSTM   | 2020 | RAVDESS, EMO-DB                               | CNN, RCNN, Proposed                         | Precision, Recall, F1 Score |\n",
    "| Feature extraction algorithm   | 2020 | RAVDESS, EMO-DB                               | SVM, MLP, DT, KNN                           | Accuracy                    |\n",
    "| SER using ML                   | 2020 | SAVEE                                         | CNN                                         | Accuracy                    |\n",
    "| SER using CNN                  | 2020 | RAVDESS, TESS                                 | CNN                                         | Accuracy                    |\n",
    "| Using speech signal            | 2019 | RML, SUSAS                                    | Random Forest, SVM                          | Precision, Accuracy         |\n",
    "| Audio-textual emotion recognition | 2019 | TESS                                         | CNN, LSTM                                   | Accuracy, Recall            |\n",
    "| Speech Recognition using MFCC  | 2020 | SAVEE, EMO-DB                                 | SVM                                         | Accuracy                    |\n",
    "| Automatic SER                  | 2019 | CREMA-D, TESS                                 | MLR, SVM                                    | Accuracy                    |\n",
    "\n",
    "# 5. Evaluation Metrics in SER\n",
    "\n",
    "SER models are evaluated using different performance metrics:\n",
    "\n",
    "- **Accuracy**: Measures overall correctness.\n",
    "- **Precision, Recall, and F1-score**: Assess class-wise performance.\n",
    "- **Confusion Matrix**: Visualizes misclassification patterns.\n",
    "- **Mean Squared Error (MSE) & Root Mean Square Error (RMSE)**: Evaluates regression-based emotion models.\n",
    "- **Weighted Accuracy (WA)**: Reflects overall accuracy but biased toward majority classes.\n",
    "- **Unweighted Accuracy (UA)**: Balances class-wise performance, critical for imbalanced datasets.\n",
    "\n",
    "## Strengths and Limitations\n",
    "\n",
    "- **Accuracy** provides a good overall performance measure but can be misleading in imbalanced datasets.\n",
    "- **Precision & Recall** are critical for assessing performance in multi-class SER problems.\n",
    "- **F1-score** balances precision and recall, making it ideal for imbalanced datasets.\n",
    "- **Confusion Matrix** allows deeper insight into misclassification tendencies.\n",
    "- **WA** is biased toward majority classes.\n",
    "- **UA** is critical for imbalanced datasets.\n",
    "\n",
    "# 6. Open Challenges and Future Opportunities\n",
    "\n",
    "- **Data Scarcity**: Limited high-quality emotional speech datasets.\n",
    "- **Cross-Language and Cross-Cultural Variability**: SER models often fail to generalize across languages.\n",
    "- **Real-World Noisy Environments**: Many models perform poorly in real-world settings with background noise.\n",
    "- **Multimodal Emotion Recognition**: Combining facial and textual cues with speech could improve accuracy.\n",
    "- **Lightweight Models for Edge Devices**: Optimizing models for deployment on low-resource hardware.\n",
    "- **Cross-Cultural Generalization**: Models trained on Western datasets (e.g., IEMOCAP) underperform on non-English languages (e.g., Bangla in SUBESCO).\n",
    "- **Noise Robustness**: Performance drops in real-world noisy environments (e.g., call centers).\n",
    "- **Data Scarcity**: Labeled emotion datasets are small and expensive to collect.\n",
    "- **Opportunity**: Self-supervised models (e.g., emotion2vec) reduce reliance on labeled data.\n",
    "- **Multimodal Fusion**: Integrating text, video, or physiological signals could improve accuracy (e.g., MDREâ€™s audio-text fusion).\n",
    "- **Ethical Concerns**: Bias in emotion labeling (e.g., cultural stereotypes) and privacy risks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "    Extracts the Mel-frequency cepstral coefficients (MFCC) from an audio file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): Path to the audio file.\n",
    "    n_mfcc (int): Number of MFCCs to return. Default is 40.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A 1D array containing the mean MFCC values.\n",
    "\n",
    "    Uses:\n",
    "    - This function is useful for audio signal processing and feature extraction in speech and music analysis.\n",
    "    - The MFCCs are commonly used in various applications such as speech recognition, speaker identification, and audio classification.\n",
    "\n",
    "    Libraries:\n",
    "    - librosa: Used for loading the audio file and extracting MFCC features.\n",
    "    - numpy: Used for computing the mean of the MFCCs.\n",
    "\"\"\"\n",
    "\n",
    "def extract_mfcc(file_path, n_mfcc=40):\n",
    "    y, sr = librosa.load(file_path, sr=22050)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    return np.mean(mfcc.T, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the CNNLSTM model class\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        # Define a 1D convolutional layer\n",
    "        self.conv1 = nn.Conv1d(in_channels=40, out_channels=64, kernel_size=3, stride=1)\n",
    "        # Define a ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        # Define an LSTM layer\n",
    "        self.lstm = nn.LSTM(64, 128, batch_first=True)\n",
    "        # Define a fully connected layer\n",
    "        self.fc = nn.Linear(128, 4)  # Assuming 4 emotion classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply the convolutional layer\n",
    "        x = self.conv1(x)\n",
    "        # Apply the ReLU activation function\n",
    "        x = self.relu(x)\n",
    "        # Apply the LSTM layer\n",
    "        x, _ = self.lstm(x)\n",
    "        # Apply the fully connected layer to the last output of the LSTM\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNLSTM()\n",
    "\n",
    "# Define the loss function (CrossEntropyLoss is suitable for classification tasks)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer (Adam optimizer with learning rate 0.001)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, train_loader, epochs=10):\n",
    "    # Loop over the number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over each batch in the training data\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            # Zero the gradients for the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass: compute the model output\n",
    "            outputs = model(batch_X)\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            # Backward pass: compute the gradients\n",
    "            loss.backward()\n",
    "            # Update the model parameters\n",
    "            optimizer.step()\n",
    "        # Print the loss for the current epoch\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch CRNN model for SER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the CRNN model class for Speech Emotion Recognition\n",
    "class SER_CRNN(nn.Module):\n",
    "    def __init__(self, n_mels=128, num_classes=5):\n",
    "        super().__init__()\n",
    "        # Define the CNN layers\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # 2D convolutional layer\n",
    "            nn.ReLU(),  # ReLU activation function\n",
    "            nn.MaxPool2d(2),  # Max pooling layer\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # Another 2D convolutional layer\n",
    "            nn.ReLU(),  # ReLU activation function\n",
    "            nn.MaxPool2d(2)  # Max pooling layer\n",
    "        )\n",
    "        # Define the GRU layer\n",
    "        self.rnn = nn.GRU(64 * (n_mels // 4), 128, bidirectional=True)\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = self.cnn(x)  # Apply CNN layers\n",
    "        x = x.permute(0, 3, 1, 2).flatten(2)  # Rearrange and flatten dimensions\n",
    "        x, _ = self.rnn(x)  # Apply GRU layer\n",
    "        x = x[:, -1, :]  # Take the output from the last time step\n",
    "        return self.fc(x)  # Apply fully connected layer\n",
    "\n",
    "model = SER_CRNN()  # Instantiate the model\n",
    "criterion = nn.CrossEntropyLoss()  # Define the loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Define the optimizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
